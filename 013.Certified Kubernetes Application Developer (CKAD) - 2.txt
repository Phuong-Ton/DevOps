Module3: Configuration - 37 Lessons
1. Define, build and modify container images 07:30
2. Practice test Docker Images
3. Commands and Arguments in Docker 07:25
4. Commands and Arguments in Kubernetes 02:40
5. A quick note on editing Pods and Deployments
6. Practice Test - Kubernetes - CKAD - Commands and Arguments
7. Solution: Commands and Arguments (Optional) 19:31
8. Environment Variables 01:08
9. ConfigMaps 05:12
10. Practice Test - Kubernetes - CKAD - ConfigMaps
11. Secrets 08:20
12. Solution: ConfigMaps (Optional) 08:43
13. A quick note about Secrets!
14. Practice Test - Kubernetes - CKAD - Secrets
15. Additional Resource
16. Solution: Secrets (Optional) 09:53
17. Demo: Encrypting Secret Data at Rest 18:47
18. Pre-requisite - Security in Docker 05:37
19. Security Contexts 01:57
20. Practice Test - Kubernetes - CKAD - Security Contexts
21. Solutions-Security Contexts 06:12
22. Resource Requirements 14:51
23.Practice Test - Kubernetes - CKAD - Resource Limits
24. Solutions-Resource Requirements 03:25
25. Service Account 14:34
26. Practice Test - Kubernetes - CKAD - Service Account
27. Solutions-Service Account 08:04
28. Stay Updated!
29. Taints and Tolerations 09:55
30. Practice Test - Taints and Toleration
31. Node Selectors Logging 03:20
32. Solution - Taints and Tolerations (Optional) 10:09
33. Node Affinity 07:08
34. Practice Test - Node Affinity
35. Solution - Node Affinity (Optional) 10:13
36. Taints & Tolerations vs Node Affinity
37. Certification Tips - Student Tips

Modual 4: Multi-Container Pods - 6 Lessons
1. Multi-Container Pods 04:34
2. Practice Tests - Kubernetes CKAD - Multi Container Pods
3. Solution - Multi-Container Pods (Optional) 15:09
4. Init Containers
5. PRACTICE TEST – INIT CONTAINERS
6. Solution – Init Containers (Optional) 07:21


Module3: Configuration - 37 Lessons
1. Define, build and modify container images 07:30 -> Ultimated Course!!!
Create my own image - Dockerfile name 'DOCKERFILE'
----------------------------------------------------
FROM Ubuntu

RUN apt-get update
RUN apt-get-install python

RUN pip install flask
RUN pip install flask-mysql

COPY . /opt/source-code

ENTRYPOINT FLASK_APP=/opt/source-code/app.py flask run
--------------------------------------------------

■ docker build DOCKERFILE -t phuongton/my-custom-app
■ docker push phuongton/my-custom-app -> DOCKER REGISTOR

2. Practice test Docker Images
$ docker images
REPOSITORY                      TAG       IMAGE ID       CREATED        SIZE
mysql                           latest    7ce93a845a8a   7 weeks ago    586MB
alpine                          latest    324bc02ae123   7 weeks ago    7.79MB
nginx                           latest    a72860cb95fd   2 months ago   188MB
nginx                           alpine    1ae23480369f   2 months ago   43.2MB
ubuntu                          latest    35a88802559d   3 months ago   78MB
redis                           latest    6c00f344e3ef   3 months ago   116MB
postgres                        latest    07a4ee949b9e   4 months ago   432MB
kodekloud/simple-webapp-mysql   latest    129dd9f67367   5 years ago    96.6MB
kodekloud/simple-webapp         latest    c6e3cd9aae36   5 years ago    84.8MB

$ cat webapp-color/Dockerfile 
FROM python:3.6

RUN pip install flask

COPY . /opt/

EXPOSE 8080

WORKDIR /opt

ENTRYPOINT ["python", "app.py"]

Move to the directory first by using the cd command and verify the path of the working directory from pwd command :-

$ cd /root/webapp-color/
$ pwd
/root/webapp-color
Now, run the docker build command within that directory :-

$ docker build -t webapp-color . 
NOTE: At the end of the command, we used the "." (dot) symbol which indicates for the current directory, so you need to run this command from within the directory that has the Dockerfile.

$docker build . -t webapp-color 

Run an instance of the image webapp-color and publish port 8080 on the container to 8282 on the host.

$docker run -p 8282:8080 webapp-color

$ docker run python:3.6 cat /etc/*release*
PRETTY_NAME="Debian GNU/Linux 11 (bullseye)"
NAME="Debian GNU/Linux"
VERSION_ID="11"
VERSION="11 (bullseye)"
VERSION_CODENAME=bullseye
ID=debian
HOME_URL="https://www.debian.org/"
SUPPORT_URL="https://www.debian.org/support"
BUG_REPORT_URL="https://bugs.debian.org/"

$ docker images
REPOSITORY                      TAG           IMAGE ID       CREATED         SIZE
webapp-color                    latest        b0185275e2d1   3 minutes ago   913MB
nginx                           latest        39286ab8a5e1   4 weeks ago     188MB
mysql                           latest        7ce93a845a8a   7 weeks ago     586MB

3. Commands and Arguments in Docker 07:25
4. Commands and Arguments in Kubernetes 02:40
5. A quick note on editing Pods and Deployments
-------------------------------------------------------------------------------------
Remember, you CANNOT edit specifications of an existing POD other than the below.

spec.containers[*].image
spec.initContainers[*].image
spec.activeDeadlineSeconds
spec.tolerations

For example, you cannot edit the environment variables, service accounts, and resource limits (all of which we will discuss later) of a running pod. But if you really want to, you have two options:

1. Run the kubectl edit pod command. This will open the pod specification in an editor (vi editor). Then, edit the required properties. When you try to save it, you will be denied. This is because you are attempting to edit a field on the pod that is not editable.
A copy of the file with your changes is saved in a temporary location, as shown above.
You can then delete the existing pod by running the command:
kubectl delete pod webapp
Then, create a new pod with your changes using the temporary file:
kubectl create -f /tmp/kubectl-edit-ccvrq.yaml

2. The second option is to extract the pod definition in YAML format to a file using the command
kubectl get pod webapp -o yaml > my-new-pod.yaml
Then, make the changes to the exported file using an editor (vi editor). Save the changes
vi my-new-pod.yaml
Then, delete the existing pod.
kubectl delete pod webapp
Then, create a new pod with the edited file.
kubectl create -f my-new-pod.yaml
Edit Deployments
With Deployments, you can easily edit any field/property of the POD template. Since the pod template is a child of the deployment specification, with every change, the deployment will automatically delete and create a new pod with the new changes. So, if you are asked to edit a property of a POD part of a deployment, you may do that simply by running the command.
kubectl edit deployment my-deployment
-------------------------------------------------------------------------------------
6. Practice Test - Kubernetes - CKAD - Commands and Arguments
Create a pod with the ubuntu image to run a container to sleep for 5000 seconds. Modify the file ubuntu-sleeper-2.yaml.
---
apiVersion: v1 
kind: Pod 
metadata:
  name: ubuntu-sleeper-2 
spec:
  containers:
  - name: ubuntu
    image: ubuntu
    command:
      - "sleep"
      - "5000"
-----------------------------------
Create a pod using the file named ubuntu-sleeper-3.yaml. There is something wrong with it. Try to fix it!


Note: Only make the necessary changes. Do not modify the name.
---
apiVersion: v1 
kind: Pod 
metadata:
  name: ubuntu-sleeper-3 
spec:
  containers:
  - name: ubuntu
    image: ubuntu
    command:
      - "sleep"
      - 1200
---------------------------------
controlplane ~/webapp-color ➜  ls
Dockerfile   Dockerfile2

controlplane ~/webapp-color ➜  cat Dockerfile
FROM python:3.6-alpine

RUN pip install flask

COPY . /opt/

EXPOSE 8080

WORKDIR /opt

ENTRYPOINT ["python", "app.py"]

controlplane ~/webapp-color ➜  

--------------------------------
Inspect the file Dockerfile2 given at /root/webapp-color directory. What command is run at container startup?
controlplane ~/webapp-color ➜  cat Dockerfile2 
FROM python:3.6-alpine

RUN pip install flask

COPY . /opt/

EXPOSE 8080

WORKDIR /opt

ENTRYPOINT ["python", "app.py"]

CMD ["--color", "red"]


"python app.py --color red" 
------------------------------------


---
apiVersion: v1 
kind: Pod 
metadata:
  name: ubuntu-sleeper-3 
spec:
  containers:
  - name: ubuntu
    image: ubuntu
    command:
      - "sleep"
      - "1200"
	  
---
apiVersion: v1 
kind: Pod 
metadata:
  name: webapp-green
  labels:
      name: webapp-green 
spec:
  containers:
  - name: simple-webapp
    image: kodekloud/webapp-color
    args: ["--color", "green"]
7. Solution: Commands and Arguments (Optional) 19:31
8. Environment Variables 01:08
9. ConfigMaps 05:12

	  
configMap
APP_COLOR: blue
APP_MODE: prod


imperative: kubectl create configmap app-config --from-literal=APP_COLOR=blue --from-literal=APP_MODE=prod
imperative: kubectl create configmap app-config --from-file=<PATH of file>


declarative: config-map.yaml
--
apiVersion: k1
kind: ConfigMaps
metadata:
  name: config-map
data:
  APP_COLOR: blue
  APP_MODE: prod

kubectl create -f  config-map.yaml

kubectl get configmaps
kubectl describe configmaps

containers:
  envFrom:
    -configMapRef
	  name: config-map
	  
or
Using on environment variables

 - env:
    - name: APP_COLOR
	  valuefrom: 
	    configMapRef
		  name: config-map
		  key: APP_COLOR
10. Practice Test - Kubernetes - CKAD - ConfigMaps
---
apiVersion: v1
kind: Pod
metadata:
  labels:
    name: webapp-color
  name: webapp-color
  namespace: default
spec:
  containers:
  - env:
    - name: APP_COLOR
      value: green
    image: kodekloud/webapp-color
    name: webapp-color
	
	
Run the command kubectl create configmap  webapp-config-map --from-literal=APP_COLOR=darkblue --from-literal=APP_OTHER=disregard
controlplane ~ ➜  kubectl create -f webapp-config-map.yaml 
configmap/webapp-config-map created

controlplane ~ ➜  cat webapp-config-map.yaml 
apiVersion: v1
kind: ConfigMap
metadata: 
  name: webapp-config-map
data:
  APP_COLOR: darkblue
  APP_OTHER: disregard

---
apiVersion: v1
kind: Pod
metadata:
  labels:
    name: webapp-color
  name: webapp-color
  namespace: default
spec:
  containers:
  - env:
    - name: APP_COLOR
      valueFrom:
       configMapKeyRef:
         name: webapp-config-map
         key: APP_COLOR
    image: kodekloud/webapp-color
    name: webapp-color
11. Secrets 08:20
controlplane ~ ✖ echo -n "sql01" | base64
c3FsMDE=

controlplane ~ ➜  echo -n "root" | base64
cm9vdA==

controlplane ~ ➜  echo -n "password123" | base64
cGFzc3dvcmQxMjM=

controlplane ~ ➜  vi sample.yaml 

controlplane ~ ➜  kubectl create -f sample.yaml 
secret/db-secret created

controlplane ~ ➜  cat sample.yaml 
apiVersion: v1
kind: Secret
metadata: 
  name: db-secret
data:
  DB_Host: c3FsMDE=
  DB_User: cm9vdA==
  DB_Password: cGFzc3dvcmQxMjM=



controlplane ~ ➜  kubectl create -f create_pod.yaml 
pod/webapp-pod created

controlplane ~ ➜  cat create_pod.yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    name: webapp-pod
  name: webapp-pod
spec:
  containers:
  - image: kodekloud/simple-webapp-mysql
    name: webapp
    envFrom:
      - secretRef:
          name: db-secret
  

12. Solution: ConfigMaps (Optional) 08:43
13. A quick note about Secrets!
Remember that secrets encode data in base64 format. Anyone with the base64 encoded secret can easily decode it. As such the secrets can be considered as not very safe.

The concept of safety of the Secrets is a bit confusing in Kubernetes. The Kubernetes documentation page and a lot of blogs out there refer to secrets as a “safer option” to store sensitive data. They are safer than storing in plain text as they reduce the risk of accidentally exposing passwords and other sensitive data. In my opinion, it’s not the secret itself that is safe, it is the practices around it.

Secrets are not encrypted, so it is not safer in that sense. However, some best practices around using secrets make it safer. As in best practices like:

Not checking in secret object definition files to source code repositories.
Enabling Encryption at Rest for Secrets so they are stored encrypted in ETCD.
Also, the way Kubernetes handles secrets. Such as:

A secret is only sent to a node if a pod on that node requires it.

Kubelet stores the secret into a tmpfs so that the secret is not written to disk storage.

Once the Pod that depends on the secret is deleted, kubelet will delete its local copy of the secret data as well.

Read about the protections and risks of using secrets here

Having said that, there are other better ways of handling sensitive data like passwords in Kubernetes, such as using tools like Helm Secrets, and HashiCorp Vault. I hope to give a lecture on these in the future.

14. Practice Test - Kubernetes - CKAD - Secrets
15. Additional Resource
Dive deep into the world of Kubernetes security with our comprehensive guide to Secret Store CSI Driver.
https://www.youtube.com/watch?v=MTnQW9MxnRI
16. Solution: Secrets (Optional) 09:53
17. Demo: Encrypting Secret Data at Rest 18:47
18. Pre-requisite - Security in Docker 05:37
19. Security Contexts 01:57
20. Practice Test - Kubernetes - CKAD - Security Contexts

kubectl exec ubuntu-sleeper -- whoami
------------------------------------------------------------------
Edit the pod ubuntu-sleeper to run the sleep process with user ID 1010.
Note: Only make the necessary changes. Do not modify the name or image of the pod.

kubectl delete po ubuntu-sleeper 
---
apiVersion: v1
kind: Pod
metadata:
  name: ubuntu-sleeper
  namespace: default
spec:
  securityContext:
    runAsUser: 1010
  containers:
  - command:
    - sleep
    - "4800"
    image: ubuntu
    name: ubuntu-sleeper
----------------------------------------------------
Update pod ubuntu-sleeper to run as Root user and with the SYS_TIME capability.


Note: Only make the necessary changes. Do not modify the name of the pod.
>>
---
apiVersion: v1
kind: Pod
metadata:
  name: ubuntu-sleeper
  namespace: default
spec:
  containers:
  - command:
    - sleep
    - "4800"
    image: ubuntu
    name: ubuntu-sleeper
    securityContext:
      capabilities:
        add: ["SYS_TIME"]
		

--------------------------------------------------------------------------------
controlplane ~ ➜  kubectl delete po ubuntu-sleeper --force
Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.
pod "ubuntu-sleeper" force deleted

controlplane ~ ➜  vi new_ubuntu.yaml

controlplane ~ ➜  kubectl create -f new_ubuntu.yaml 
pod/ubuntu-sleeper created

controlplane ~ ➜  cat new_ubuntu.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: ubuntu-sleeper
  namespace: default
spec:
  containers:
  - command:
    - sleep
    - "4800"
    image: ubuntu
    name: ubuntu-sleeper
    securityContext:
      capabilities:
        add: ["SYS_TIME","NET_ADMIN"]

21. Solutions-Security Contexts 06:12
22. Resource Requirements 14:51
23.Practice Test - Kubernetes - CKAD - Resource Limits


The elephant pod runs a process that consumes 15Mi of memory. Increase the limit of the elephant pod to 20Mi.


Delete and recreate the pod if required. Do not modify anything other than the required fields.
->

Create the file elephant.yaml by running command kubectl get po elephant -o yaml > elephant.yaml and edit the file such as memory limit is set to 20Mi as follows:
---
apiVersion: v1
kind: Pod
metadata:
  name: elephant
  namespace: default
spec:
  containers:
  - args:
    - --vm
    - "1"
    - --vm-bytes
    - 15M
    - --vm-hang
    - "1"
    command:
    - stress
    image: polinux/stress
    name: mem-stress
    resources:
      limits:
        memory: 20Mi
      requests:
        memory: 5Mi
		
24. Solutions-Resource Requirements 03:25
25. Service Account 14:34
■  kubectl create serviceaccount my-service_acc
create token + object , and secret

controlplane ~ ✖ kubectl create serviceaccount dashboard-sa
serviceaccount/dashboard-sa created

controlplane ~ ➜  cat /var/rbac
cat: read error: Is a directory

controlplane ~ ✖ ls  /var/rbac
dashboard-sa-role-binding.yaml  pod-reader-role.yaml

controlplane ~ ➜  cat /var/rbac/pod-reader-role.yaml 
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: default
  name: pod-reader
rules:
- apiGroups:
  - ''
  resources:
  - pods
  verbs:
  - get
  - watch
  - list

controlplane ~ ➜  cat /var/rbac/dashboard-sa-role-binding.yaml 
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: read-pods
  namespace: default
subjects:
- kind: ServiceAccount
  name: dashboard-sa # Name is case sensitive
  namespace: default
roleRef:
  kind: Role #this must be Role or ClusterRole
  name: pod-reader # this must match the name of the Role or ClusterRole you wish to bind to
  apiGroup: rbac.authorization.k8s.io
  
  
  
kubectl create token dashboard-sa  then copy it to webapi for getting data from Kubernetes
-----------------------------------------
You shouldn't have to copy and paste the token each time. The Dashboard application is programmed to read token from the secret mount location. However currently, the default service account is mounted. Update the deployment to use the newly created ServiceAccount


Edit the deployment to change ServiceAccount from default to dashboard-sa.
->>>
Use the kubectl edit command for the deployment and specify the serviceAccountName field inside the pod spec.
OR

Make use of the kubectl set command. Run the following command to use the newly created service account: - kubectl set serviceaccount deploy/web-dashboard dashboard-sa

apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-dashboard
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      name: web-dashboard
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        name: web-dashboard
    spec:
      serviceAccountName: dashboard-sa
      containers:
      - image: gcr.io/kodekloud/customimage/my-kubernetes-dashboard
        imagePullPolicy: Always
        name: web-dashboard
        ports:
        - containerPort: 8080
          protocol: TCP
		  
26. Practice Test - Kubernetes - CKAD - Service Account
27. Solutions-Service Account 08:04
28. Stay Updated!
To stay up to date about our new courses and discussions:

Follow me on Twitter: @mmumshad
https://twitter.com/x/migrate?tok=7b2265223a222f6d6d756d73686164222c2274223a313732363539333432327d05fa5b3eb929cb4ae974ba6409ca5e8b
Subscribe to our Youtube Channel
https://www.youtube.com/user/mmumshad?sub_confirmation=1
Join our Facebook Group
https://www.facebook.com/kodekloudtraining


29. Taints and Tolerations 09:55
Taint node and tolerate pod to schedule pod to the node
kubectl taint nodes node-name key=value:taints-effect
taints-effect: NoSchedule, PreferNoSchedule, and NoExecute
NoSchedule: pod will not be schedule on the node
PreferNoSchedule: the system will try to avoid placing a pod on then node, but that not guaranteed
NoExecute: new node will be not schedule on the node and existing node is not execute
------------------------------------------------------
Taint node
------------------------------------------------------
kubectl taint nodes node1 app=blue:NoSchedule
------------------------------------------------------
Toleration POD at spec: in definition file
------------------------------------------------------
spec:
  tolerations:
    - key: "app"
	  operator: "Equal"
	  value: "blue"
	  effect: "NoSchedule"

Master node is automatically tainted to avoid pod schedule on master node

30. Practice Test - Taints and Toleration
1. Do any taints exist on node01 node?
Run the command: kubectl describe node node01 | grep -i taints to check taint exists.

2. Create a taint on node01 with key of spray, value of mortein and effect of NoSchedule

kubectl taint nodes node01 spray=value:NoSchedule
kubectl taint nodes node01 spray=mortein:NoSchedule --overwrite

3. Create a new pod with the nginx image and pod name as mosquito.
kubectl run mosquito --image=nginx
---
apiVersion: v1
kind: Pod
metadata:
  name: mosquito
spec:
  containers:
  - image: nginx
    name: mosquito
then run kubectl create -f <FILE-NAME>.yaml
-> pendingstatus

4. Create another pod named bee with the nginx image, which has a toleration set to the taint mortein
controlplane ~ ➜  vi bee_pod.yaml

controlplane ~ ➜  cat bee_pod.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: bee
spec:
  containers:
  - image: nginx
    name: bee
  tolerations:
    - key: "spray"
      operator: "Equal"
      value: "mortein"
      effect: "NoSchedule"

controlplane ~ ➜  kubectl create -f bee_pod.yaml 
pod/bee created
4. Remove the taint on controlplane, which currently has the taint effect of NoSchedule. "-" add minus after command taint
Run the command: kubectl taint nodes controlplane node-role.kubernetes.io/control-plane:NoSchedule-
controlplane ~ ➜  kubectl describe node controlplane | grep -i taint
Taints:             node-role.kubernetes.io/control-plane:NoSchedule

controlplane ~ ➜  kubectl get nodes
NAME           STATUS   ROLES           AGE   VERSION
controlplane   Ready    control-plane   17m   v1.30.0
node01         Ready    <none>          16m   v1.30.0

controlplane ~ ➜  kubectl taint nodes controlplane node-role.kubernetes.io/control-plane:NoSchedule
error: node controlplane already has node-role.kubernetes.io/control-plane taint(s) with same effect(s) and --overwrite is false

controlplane ~ ✖ kubectl taint nodes controlplane node-role.kubernetes.io/control-plane:NoSchedule-
node/controlplane untainted

controlplane ~ ➜  kubectl describe node controlplane | grep -i taint
Taints:             <none>

31. Node Selectors Logging 03:20
Label Node command
kubectl label nodes <node name> <label key>=<label value>
example: kubectl lable nodes node1 size=Large

at pod definition file
spec:
  nodeSelector:
    size: Large

32. Solution - Taints and Tolerations (Optional) 10:09
33. Node Affinity 07:08
Large and medium, NOT Small expresion can support by Node Affinity through advance capabilities

at pod definition file 

spec:
  affinity:
    nodeAffinity:
	  requiredDuringSchedulingIgnoreDuringExecution:
	    nodeSelectorTerms:
		  - matchExpressions:
		      key: "size"
			  operator: "In"
			  value: 
			   - Large
			   - Medium
			   
		  - matchExpressions:
		      key: "size"
			  operator: "NotIn"
			  value: 
			   - Small
			   	  
		  - matchExpressions:
		      key: "size"
			  operator: "Exists"

33.1. requiredDuringSchedulingIgnoreDuringExecution
33.2. preferredDuringSchedulingIgnoreDuringExecution
33.3. requiredDuringSchedulingrequiredDuringExecution

DuringScheduling: first time create pod
DuringExecutiong: pod exist on the node

required: must match
preferred: if matching node not found, scheduler will ignorge this rule and scheduler available node to add this pod-> try your best to find nodes but not find push it anywhere
 
If you can't explain it simply you don't understand it well enough.


34. Practice Test - Node Affinity

example: kubectl label nodes node1 size=Large

1. Apply a label color=blue to node node01

kubectl label nodes node01 color=blue

2. 
Create a new deployment named blue with the nginx image and 3 replicas.

Run the command: kubectl create deployment blue --image=nginx --replicas=3


controlplane ~ ➜  kubectl create -f deploy.yaml 
deployment.apps/blue created

controlplane ~ ➜  cat deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: blue
  labels:
    name: blue
    app: app-blue
spec:
  replicas: 3
  selector:
    matchLabels:
      name: nginx-pod
      app: app-blue
  template:
    metadata:
      name: nginx-pod
      labels:
        name: nginx-pod
        app: app-blue
    spec:
      containers:
        - name: blue-nginx
          image: nginx
		  

Check if controlplane and node01 have any taints on them that will prevent the pods to be scheduled on them. If there are no taints, the pods can be scheduled on either node.

So run the following command to check the taints on both nodes.

kubectl describe node controlplane | grep -i taints

kubectl describe node node01 | grep -i taints


---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: blue
spec:
  replicas: 3
  selector:
    matchLabels:
      run: nginx
  template:
    metadata:
      labels:
        run: nginx
    spec:
      containers:
      - image: nginx
        imagePullPolicy: Always
        name: nginx
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: color
                operator: In
                values:
                - blue
apiVersion: apps/v1
kind: Deployment
metadata:
  name: blue
  labels:
    name: blue
    app: app-blue
spec:
  replicas: 3
  selector:
    matchLabels:
      name: nginx-pod
      app: app-blue
  template:
    metadata:
      name: nginx-pod
      labels:
        name: nginx-pod
        app: app-blue
    spec:
      containers:
        - name: blue-nginx
          image: nginx
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                - key: color
                  operator: In
                  values:
                    - blue


Run the command: kubectl get pods -o wide and see the Node column.

3. Create a new deployment named red with the nginx image and 2 replicas, and ensure it gets placed on the controlplane node only.
Use the label key - node-role.kubernetes.io/control-plane - which is already set on the controlplane node.

                    node-role.kubernetes.io/control-plane=

controlplane ~ ➜  kubectl create -f red_deploy.yaml 
deployment.apps/red created

controlplane ~ ➜  cat red_deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: red
  labels:
    name: red
    app: app-red
spec:
  replicas: 2
  selector:
    matchLabels:
      name: nginx-red-pod
      app: app-red
  template:
    metadata:
      name: nginx-red-pod
      labels:
        name: nginx-red-pod
        app: app-red
    spec:
      containers:
        - name: red-nginx
          image: nginx
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                - key: 'node-role.kubernetes.io/control-plane'
                  operator: "Exist

35. Solution - Node Affinity (Optional) 10:13
Make sure you check out these tips and tricks from other students who have cleared the exam: https://medium.com/@harioverhere/ckad-certified-kubernetes-application-developer-my-journey-3afb0901014


36. Taints & Tolerations vs Node Affinity
Blue Red Green NoColor NoColor 


Node-Red Node-Blue Node-Green Node01

-> Step 1: create Label color=red, color=blue, color=green on  Node-Red Node-Blue Node-Green
kubectl label nodes Node-Blue color=blue

-> Step 2: edit deploy.yaml at template: spec:
spec:
	template: 
		spec:
		  affinity:
			nodeAffinity:
			  requiredDuringSchedulingIgnoredDuringExecution:
				nodeSelectorTerms:
				  - matchExpressions:
					- key: 'color'
					  operator: "In"
					  values:
						- blue
	--> Step 3: set up taint NoSchedule on Node-Red Node-Blue Node-Green
kubectl taint nodes Node-blue spray=color:NoSchedule


--> Step 4: setup toleration on deploy definition template: spec:
spec:
	template:
		spec:
		  tolerations:
			- key: "spray"
			  operator: "Equal"
			  value: "color"
			  effect: "NoSchedule"
37. Certification Tips - Student Tips




