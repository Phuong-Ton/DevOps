Module 5: Observability
5.1 Readiness Probes 08:11 ‚òÖ
5.2 Liveness Probes 03:19 ‚òÖ
5.3 Practice Test - Kubernetes - CKAD - Readiness Probes
5.4 Solution-Readiness and Liveness Probes 09:30
5.5 Logging 02:35
5.6 Practice Test - Kubernetes - CKAD - Logging
5.7 Solution - Logging (optional) 02:48
5.8 Monitoring 04:14
5.9 Practice Test - Kubernetes - CKAD - Monitoring
5.10 Solution - Monitoring (optional) 04:21

5.1 Readiness Probes 08:11 ‚òÖ
If the readiness probe returns a failed state, Kubernetes removes the pod from all matching service endpoints.
Readiness probes runs on the container during its whole lifecycle

spec:
  containers:
    readinessProbe:
      tcpSocket:
        port: 8080
      initialDelaySeconds: 15
      periodSeconds: 10
    livenessProbe:
      tcpSocket:
        port: 8080
      initialDelaySeconds: 15
      periodSeconds: 10
	  
5.2 Liveness Probes 03:19 ‚òÖ
when an application is running, but unable to make progress
--> need restart
Liveness probesdetermine when to restart a container
--> if Liveness probes fail -> the kubelet restarts the container.
--> If you want to wait before executing a liveness probe you can either define initialDelaySeconds

spec:
  containers:
    livenessProbe:
      exec:
        command:
        - cat
        - /tmp/healthy
      initialDelaySeconds: 5
      periodSeconds: 5
----------------------------------------
spec:
  containers:
    livenessProbe:
      httpGet:
        path: /healthz
        port: 8080
        httpHeaders:
        - name: Custom-Header
          value: Awesome
      initialDelaySeconds: 3
      periodSeconds: 3
5.3 Practice Test - Kubernetes - CKAD - Readiness Probes
5.4 Solution-Readiness and Liveness Probes 09:30
01. Inspec Pods and service. After that, run script
controlplane ~ ‚ûú  alias k=kubectl

controlplane ~ ‚ûú  k get svc pod
Error from server (NotFound): services "pod" not found

controlplane ~ ‚úñ k get svc,pod
NAME                     TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
service/kubernetes       ClusterIP   10.96.0.1      <none>        443/TCP          5m29s
service/webapp-service   NodePort    10.102.48.11   <none>        8080:30080/TCP   25s

NAME                  READY   STATUS    RESTARTS   AGE
pod/simple-webapp-1   1/1     Running   0          25s

controlplane ~ ‚ûú  pwd
/root

controlplane ~ ‚ûú  ls
crash-app.sh  curl-test.sh  freeze-app.sh

controlplane ~ ‚ûú  cat curl-test.sh 
for i in {1..20}; do
   kubectl exec --namespace=kube-public curl -- sh -c 'test=`wget -qO- -T 2  http://webapp-service.default.svc.cluster.local:8080/ready 2>&1` && echo "$test OK" || echo "Failed"';
   echo ""
done
controlplane ~ ‚ûú  ./curl-test.sh 
Message from simple-webapp-1 : I am ready! OK

Message from simple-webapp-1 : I am ready! OK

Message from simple-webapp-1 : I am ready! OK

02. We just created a new POD to scale the application. View the pods. Run the script again and view the results.

The application takes 80 seconds to warm up.
controlplane ~ ‚ûú  k get svc,pod
NAME                     TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
service/kubernetes       ClusterIP   10.96.0.1      <none>        443/TCP          7m2s
service/webapp-service   NodePort    10.102.48.11   <none>        8080:30080/TCP   118s

NAME                  READY   STATUS    RESTARTS   AGE
pod/simple-webapp-1   1/1     Running   0          118s
pod/simple-webapp-2   1/1     Running   0          12s

controlplane ~ ‚ûú  ./curl-test.sh 
Failed
Message from simple-webapp-1 : I am ready! OK
Failed
Message from simple-webapp-1 : I am ready! OK
Failed
Message from simple-webapp-1 : I am ready! OK
Message from simple-webapp-1 : I am ready! OK

-------------------------------------------------------
03.Update the newly created pod 'simple-webapp-2' with a readinessProbe using the given spec
Spec is given on the below. Do not modify any other properties of the pod.

Pod Name: simple-webapp-2
Image Name: kodekloud/webapp-delayed-start
Readiness Probe: httpGet
Http Probe: /ready
Http Port: 8080

controlplane ~ ‚ûú  kubectl get pod simple-webapp-2 -o yaml > webapp2.yaml

controlplane ~ ‚ûú  cat webapp2.yaml 
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: "2024-09-19T17:20:38Z"
  labels:
    name: simple-webapp
  name: simple-webapp-2
  namespace: default
  resourceVersion: "1011"
  uid: 774a2750-2b74-4abc-85c2-f7833ce6a8ee
spec:
  containers:
  - env:
    - name: APP_START_DELAY
      value: "80"
    image: kodekloud/webapp-delayed-start
    imagePullPolicy: Always
    name: simple-webapp
    ports:
    - containerPort: 8080
      protocol: TCP
    resources: {}
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-4txxq
      readOnly: true
-> add
    readinessProbe:
      httpGet:
        path: /ready
        port: 8080

controlplane ~ ‚ûú  vi webapp2.yaml 

controlplane ~ ‚ûú  kubectl delete pod simple-webapp-2
pod "simple-webapp-2" deleted

controlplane ~ ‚ûú  kubectl create -f webapp2.yaml 
pod/simple-webapp-2 created
-------------------------------------------------------------------------------
04. Run the script again to test the web application. Notice that none of the requests fail now. Though all the requests hit the same POD.
Execute the script at /root/curl-test.sh.

-------------------------------------------------------------------------------
controlplane ~ ‚ûú  ./curl-test.sh 
Message from simple-webapp-1 : I am ready! OK
Message from simple-webapp-1 : I am ready! OK
Message from simple-webapp-2 : I am ready! OK
Message from simple-webapp-1 : I am ready! OK
Message from simple-webapp-2 : I am ready! OK

-----------------------------------------------------------------------------------------
05.What would happen if the application inside container on one of the PODs crashes?

Try it by accessing url /crash of the application in your browser or run the crash-app.sh script. Then check the status of POD. Run the curl-test to see if users are impacted

controlplane ~ ‚ûú  kubectl get pods
NAME              READY   STATUS    RESTARTS      AGE
simple-webapp-1   1/1     Running   2 (19s ago)   13m
simple-webapp-2   1/1     Running   0             4m4s

controlplane ~ ‚ûú  cat crash-app.sh 
kubectl exec --namespace=kube-public curl -- wget -qO- http://webapp-service.default.svc.cluster.local:8080/crash

controlplane ~ ‚ûú  ./crash-app.sh 
Message from simple-webapp-1 : Mayday! Mayday! Going to crash!
controlplane ~ ‚ûú  ./curl-test.sh 
Message from simple-webapp-2 : I am ready! OK
Message from simple-webapp-1 : I am ready! OK
Message from simple-webapp-1 : I am ready! OK
-> RESTART

------------------------------------------------------------------------
06. Update both the pods with a livenessProbe using the given spec
Delete and recreate the PODs.

Pod Name: simple-webapp-1
Image Name: kodekloud/webapp-delayed-start
Liveness Probe: httpGet
Http Probe: /live
Http Port: 8080
Period Seconds: 1
Initial Delay: 80
Pod Name: simple-webapp-2
Image Name: kodekloud/webapp-delayed-start
Liveness Probe: httpGet
Http Probe: /live
Http Port: 8080
Initial Delay: 80
Period Seconds: 1

controlplane ~ ‚ûú  kubectl get pods -o yaml > web1-2.yaml
controlplane ~ ‚ûú  vi web1-2.yaml 
      livenessProbe:
        httpGet:
          path: /live
          port: 8080
        initialDelaySeconds: 80
        periodSeconds: 1
		
controlplane ~ ‚ûú  kubectl delete pod --all
pod "simple-webapp-1" deleted
pod "simple-webapp-2" deleted
controlplane ~ ‚ûú  kubectl create -f web1-2.yaml 
pod/simple-webapp-1 created
pod/simple-webapp-2 created

5.5 Logging 02:35
5.6 Practice Test - Kubernetes - CKAD - Logging
5.7 Solution - Logging (optional) 02:48
‚ú∞ Log on docker:
‚ñ† docker logs -f ecf
--> see live logs

‚ú∞ log on kubernetes:
‚ñ† kubectl logs -f <pod name> <name container if it has multiple containers> 
-f to stream a logs live

‚ñ† kubectl logs -f even-simulator-pod event-simulator

5.8 Monitoring 04:14
5.9 Practice Test - Kubernetes - CKAD - Monitoring
5.10 Solution - Monitoring (optional) 04:21
5.11 Feedback
‚ñ† minikube addons enable metrics-server
-> monitoring cpu disk and mem on 
‚ñ† kubectl create -f deploy/1.8+/
‚ñ† kubectl top node
‚ñ† kubectl top pod

01. Get info of kubernetes cluster
controlplane ~ ‚ûú  alias k=kubectl
controlplane ~ ‚ûú  k get pod
NAME       READY   STATUS    RESTARTS   AGE
elephant   1/1     Running   0          20s
lion       1/1     Running   0          20s
rabbit     1/1     Running   0          20s

controlplane ~ ‚ûú  k get service
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   9m14s

02. Run: git clone https://github.com/kodekloudhub/kubernetes-metrics-server.git

controlplane ~ ‚ûú  pwd
/root

controlplane ~ ‚ûú  ls
sample.yaml

controlplane ~ ‚ûú  git clone https://github.com/kodekloudhub/kubernetes-metrics-server.git
Cloning into 'kubernetes-metrics-server'...
remote: Enumerating objects: 31, done.
remote: Counting objects: 100% (19/19), done.
remote: Compressing objects: 100% (19/19), done.
remote: Total 31 (delta 8), reused 0 (delta 0), pack-reused 12 (from 1)
Receiving objects: 100% (31/31), 8.08 KiB | 8.08 MiB/s, done.
Resolving deltas: 100% (10/10), done.

controlplane ~ ‚ûú  ls
kubernetes-metrics-server  sample.yaml

controlplane ~ ‚ûú  ls kubernetes-metrics-server/
aggregated-metrics-reader.yaml  auth-reader.yaml         metrics-server-deployment.yaml  README.md
auth-delegator.yaml             metrics-apiservice.yaml  metrics-server-service.yaml     resource-reader.yam

03. Deploy the metrics-server by creating all the components downloaded.
Run the kubectl create -f . command from within the downloaded repository.

controlplane ~ ‚ûú  cd kubernetes-metrics-server/

controlplane kubernetes-metrics-server on ÓÇ† master ‚ûú  pwd
/root/kubernetes-metrics-server

controlplane kubernetes-metrics-server on ÓÇ† master ‚ûú  ls
aggregated-metrics-reader.yaml  auth-reader.yaml         metrics-server-deployment.yaml  README.md
auth-delegator.yaml             metrics-apiservice.yaml  metrics-server-service.yaml     resource-reader.yaml

controlplane kubernetes-metrics-server on ÓÇ† master ‚ûú  kubectl create -f .
clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created
clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created
rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created
apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created
serviceaccount/metrics-server created
deployment.apps/metrics-server created
service/metrics-server created
clusterrole.rbac.authorization.k8s.io/system:metrics-server created
clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created
‚ñ† kubectl top node
‚ñ† kubectl top pod

Module 6: POD Designüüä
6.1 Labels, Selectors & Annotations
6.2 Practice Test - Kubernetes - CKAD - Labels and Selectors
6.3 Solution - Labels and Selectors (optional) 05:33
6.4 Rolling Updates & Rollbacks 06:42
6.5 Updating a Deployment
6.6 Practice Test - Kubernetes - CKAD - Rolling Updates and Rollbacks
6.7 Solution: Rolling Updates and Rollbacks (optional) 09:05
6.8 Deployment Strategy - Blue Green 04:35
6.9 Deployment Strategy - Canary 05:21
6.10 Practice Test - Deployment strategies
6.11 Solution: Deployment strategies 05:49
6.12 Jobs 08:07
6.13 Cron Jobs 01:45
6.14 Practice Test - Kubernetes - CKAD - Jobs and CronJobs
6.15 Solution - Jobs and Cronjobs (optional) 09:07


6.1 Labels, Selectors & Annotations
Labels is a method to group things together, classibility
Selectors is the way to filter and find objects

kubernetes: a lot of type of objects: pods, services,deployment, node, configmap,serviceaccount, ...
-> group by objects type, application(app1, app2), funtionality (frontend, backend, auth, webserver, dbserver, image-process)
‚ñ†  kubectl get pods --selector app=my-app
example: group 3 pod have label app=my-app to deployment deploy-my-app
Annotations: detail for infomation
annotations:
  buildversion: 1.3

6.2 Practice Test - Kubernetes - CKAD - Labels and Selectors
6.3 Solution - Labels and Selectors (optional) 05:33
6.4 Rolling Updates & Rollbacks 06:42
6.5 Updating a Deployment
Here are some handy examples related to updating a Kubernetes Deployment:
- Creating a deployment, checking the rollout status and history:
In the example below, we will first create a simple deployment and inspect the rollout status and the rollout history:
‚ñ† kubectl create deployment nginx --image=nginx:1.16
deployment.apps/nginx created

‚ñ† kubectl rollout status deployment nginx
Waiting for deployment "nginx" rollout to finish: 0 of 1 updated replicas are available...
deployment "nginx" successfully rolled out

‚ñ†  kubectl rollout history deployment nginx
deployment.apps/nginx 
REVISION  CHANGE-CAUSE
1          <none>    

- Using the ‚Äì-revision flag:
Here revision 1 is the first version where the deployment was created.
You can check the status of each revision individually by using the ‚Äì-revision flag:
‚ñ†  kubectl rollout history deployment nginx --revision=1
deployment.apps/nginx with revision #1

Pod Template:
Labels:       app=nginx
pod-template-hash=78449c65d4
Containers:
nginx:
Image:      nginx:1.16

- Using the ‚Äì-record flag:
You would have noticed that the ‚Äúchange-cause‚Äù field is empty in the rollout history output. We can use the ‚Äì -record flag to save the command used to create/update a deployment against the revision number.
‚ñ†  kubectl set image deployment nginx nginx=nginx:1.17 --record

Flag --record has been deprecated, --record will be removed in the future
deployment.apps/nginx image updated

‚ñ† kubectl rollout history deployment nginx
deployment.apps/nginx 
REVISION  CHANGE-CAUSE
1         <none> 
2         kubectl set image deployment nginx nginx=nginx:1.17 --record=true

- You can now see that the change-cause is recorded for revision 2 of this deployment.
Let's make some more changes. In the example below, we are editing the deployment and changing the image from nginx:1.17 to nginx:latest while making use of the ‚Äìrecord flag.

‚ñ† kubectl edit deployments.apps nginx --record
Flag --record has been deprecated, --record will be removed in the future
deployment.apps/nginx edited

‚ñ† kubectl rollout history deployment nginx
deployment.apps/nginx 
REVISION  CHANGE-CAUSE
1         <none> 
2         kubectl set image deployment nginx nginx=nginx:1.17 --record=true
3         kubectl edit deployments.apps nginx --record=true

‚ñ†  kubectl rollout history deployment nginx --revision=3
deployment.apps/nginx with revision #3
Pod Template:
  Labels:       app=nginx
        pod-template-hash=787f54657b
  Annotations:  kubernetes.io/change-cause: kubectl edit deployments.apps nginx --record=true
  Containers:
   nginx:
    Image:      nginx

- Undo a change: Let's now rollback to the previous revision:
controlplane $ kubectl rollout history deployment nginx
deployment.apps/nginx 
REVISION  CHANGE-CAUSE
1         
3         kubectl edit deployments.apps nginx --record=true
4         kubectl set image deployment nginx nginx=nginx:1.17 --record=true

‚ñ†  kubectl rollout history deployment nginx --revision=3
deployment.apps/nginx with revision #3
Pod Template:
  Labels:       app=nginx
        pod-template-hash=787f54657b
  Annotations:  kubernetes.io/change-cause: kubectl edit deployments.apps nginx --record=true
  Containers:
   nginx:
    Image:      nginx:latest

‚ñ† kubectl describe deployments. nginx | grep -i image:
    Image:        nginx:1.17

With this, we have rolled back to the previous version of the deployment with the image = nginx:1.17.
‚ñ†  kubectl rollout history deployment nginx --revision=1
deployment.apps/nginx with revision #1
Pod Template:
  Labels:       app=nginx
        pod-template-hash=78449c65d4
  Containers:
   nginx:
    Image:      nginx:1.16

‚ñ† kubectl rollout undo deployment nginx --to-revision=1
deployment.apps/nginx rolled back
To rollback to specific revision we will use the --to-revision flag.

With --to-revision=1, it will be rolled back with the first image we used to create a deployment as we can see in the rollout history output.

‚ñ† kubectl describe deployments. nginx | grep -i image:
Image: nginx:1.16

6.6 Practice Test - Kubernetes - CKAD - Rolling Updates and Rollbacks
6.7 Solution: Rolling Updates and Rollbacks (optional) 09:05
01. Inspect:
controlplane ~ ‚úñ kubectl get pods,deploy
NAME                           READY   STATUS    RESTARTS   AGE
pod/frontend-f8d68f98f-xgctb   1/1     Running   0          2m59s
pod/frontend-f8d68f98f-bfrnf   1/1     Running   0          2m59s
pod/frontend-f8d68f98f-lvs7d   1/1     Running   0          2m59s
pod/frontend-f8d68f98f-74vg6   1/1     Running   0          2m59s

NAME                       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/frontend   4/4     4            4           2m59s

controlplane ~ ‚ûú  kubectl describe deploy 
Name:                   frontend
Namespace:              default
CreationTimestamp:      Thu, 19 Sep 2024 18:43:33 +0000
Labels:                 <none>
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               name=webapp
Replicas:               4 desired | 4 updated | 4 total | 4 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        20
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  name=webapp
  Containers:
   simple-webapp:
    Image:         kodekloud/webapp-color:v1
    Port:          8080/TCP
    Host Port:     0/TCP
    Environment:   <none>
    Mounts:        <none>
  Volumes:         <none>
  Node-Selectors:  <none>
  Tolerations:     <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   frontend-f8d68f98f (4/4 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  4m3s  deployment-controller  Scaled up replica set frontend-f8d68f98f to 4
-------------------------------------------------------------------------

02.Let us try that. Upgrade the application by setting the image on the deployment to kodekloud/webapp-color:v2
Do not delete and re-create the deployment. Only set the new image name for the existing deployment
Deployment Name: frontend
Deployment Image: kodekloud/webapp-color:v2

controlplane ~ ‚úñ kubectl rollout history deploy frontend
deployment.apps/frontend 
REVISION  CHANGE-CAUSE
1         <none>

      - image: kodekloud/webapp-color:v1 ->       - image: kodekloud/webapp-color:v2

controlplane ~ ‚ûú  kubectl rollout history deploy frontend
deployment.apps/frontend 
REVISION  CHANGE-CAUSE
1         <none>
2         <none>

ontrolplane ~ ‚ûú  ./curl-test.sh 
Hello, Application Version: v1 ; Color: blue OK

Hello, Application Version: v1 ; Color: blue OK

Hello, Application Version: v2 ; Color: green OK

Hello, Application Version: v2 ; Color: green OK

Hello, Application Version: v1 ; Color: blue OK

Hello, Application Version: v2 ; Color: green OK

Hello, Application Version: v2 ; Color: green OK

Hello, Application Version: v2 ; Color: green OK

Hello, Application Version: v1 ; Color: blue OK

Hello, Application Version: v2 ; Color: green OK

Hello, Application Version: v2 ; Color: green OK

Hello, Application Version: v1 ; Color: blue OK

Hello, Application Version: v2 ; Color: green OK

Hello, Application Version: v2 ; Color: green OK

Hello, Application Version: v2 ; Color: green OK

Hello, Application Version: v2 ; Color: green OK

Hello, Application Version: v1 ; Color: blue OK

Hello, Application Version: v2 ; Color: green OK

Hello, Application Version: v1 ; Color: blue OK

Hello, Application Version: v2 ; Color: green OK

Hello, Application Version: v2 ; Color: green OK

Hello, Application Version: v2 ; Color: green OK

Hello, Application Version: v2 ; Color: green OK

Hello, Application Version: v2 ; Color: green OK

Hello, Application Version: v2 ; Color: green OK

Hello, Application Version: v2 ; Color: green OK

Hello, Application Version: v1 ; Color: blue OK

Hello, Application Version: v2 ; Color: green OK

Hello, Application Version: v2 ; Color: green OK

Hello, Application Version: v2 ; Color: green OK

Hello, Application Version: v2 ; Color: green OK

Hello, Application Version: v2 ; Color: green OK

Hello, Application Version: v2 ; Color: green OK

Hello, Application Version: v2 ; Color: green OK

Hello, Application Version: v2 ; Color: green OK


controlplane ~ ‚ûú  kubectl get pods,deploy
NAME                            READY   STATUS        RESTARTS   AGE
pod/frontend-69b69fcc6d-mbf42   1/1     Running       0          49s
pod/frontend-69b69fcc6d-5lm89   1/1     Running       0          49s
pod/frontend-f8d68f98f-lvs7d    1/1     Terminating   0          11m
pod/frontend-f8d68f98f-bfrnf    1/1     Terminating   0          11m
pod/frontend-69b69fcc6d-f5kl8   1/1     Running       0          26s
pod/frontend-69b69fcc6d-748hk   1/1     Running       0          26s
pod/frontend-f8d68f98f-74vg6    1/1     Terminating   0          11m

NAME                       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/frontend   4/4     4            4           11m

controlplane ~ ‚ûú  kubectl get pods,deploy
NAME                            READY   STATUS        RESTARTS   AGE
pod/frontend-69b69fcc6d-mbf42   1/1     Running       0          62s
pod/frontend-69b69fcc6d-5lm89   1/1     Running       0          62s
pod/frontend-69b69fcc6d-f5kl8   1/1     Running       0          39s
pod/frontend-69b69fcc6d-748hk   1/1     Running       0          39s
pod/frontend-f8d68f98f-74vg6    1/1     Terminating   0          11m


-----------------------------------------------------------------------
Change the deployment strategy to Recreate
Delete and re-create the deployment if necessary. Only update the strategy type for the existing deployment.

Deployment Name: frontend
Deployment Image: kodekloud/webapp-color:v2
Strategy: Recreate
  strategy:
    type: Recreate

6.8 Deployment Strategy - Blue Green 04:35
--------------------------------------------------------------------------------------------
Deployment Strategy:
1. Recreate: remove all old, replace all new
2. RollingUpdate: take down the older version, and bring up new version one by one
3. Blue/Green:
old version: Blue
new version: Green
new version deployed alongside the old version
      Blue                          Green
app:1.0    app:1.0           app:2.0     app:2.0

100% traffic is accessed by Blue. When passing all the tests, we switch to Green
USING LABELS to Switch from Blue to Green

Service - spec: > selector: > matchLabels: version: v1 ---> change to version: v2

Blue    - spec: > template: > metadata: > labels: > version: v1
Blue    - spec: > selector: > matchLabels: > version: v1

Green   - spec: > template: > metadata: > labels: > version: v2
Green   - spec: > selector: > matchLabels: > version: v2
--------------------------------------------------------------------------------------------
4. Canary: 

small percentage routed to the new version
      Deploy                        Canary
app:1.0    app:1.0           app:1.0     app:2.0
after test app:2.0 ok -> replace all 1.0 - > 2.0 using RollingUpdate Strategy
How to implement it:
Service - spec: > selector: > matchLabels: app: frontend 

Primary - spec: > replicas: 5
Primary - spec: > template: > metadata: > labels: > version: v1
                                                  > app: frontend
Primary - spec: > selector: > matchLabels: > version: v1
                                           > app: frontend
Canary  - spec: > replicas: 1
Canary  - spec: > template: > metadata: > labels: > version: v2
                                                  > app: frontend
Canary  - spec: > selector: > matchLabels: > version: v2
                                           > app: frontend

--> reduce number of pods in Canary to 1


6.9 Deployment Strategy - Canary 05:21
6.10 Practice Test - Deployment strategies
6.11 Solution: Deployment strategies 05:49
-----------------------------------------------------------------------------------------
1. Inspect:
controlplane ~ ‚ûú  kubectl get pod,deploy
NAME                            READY   STATUS    RESTARTS   AGE
pod/frontend-6fd794f9f9-b5w5w   1/1     Running   0          32s
pod/frontend-6fd794f9f9-bbgmv   1/1     Running   0          32s
pod/frontend-6fd794f9f9-jw6zm   1/1     Running   0          32s
pod/frontend-6fd794f9f9-mfjbg   1/1     Running   0          32s
pod/frontend-6fd794f9f9-wcx47   1/1     Running   0          32s

NAME                       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/frontend   5/5     5            5           33s

controlplane ~ ‚ûú  kubectl describe deployments frontend 
Name:                   frontend
Namespace:              default
CreationTimestamp:      Thu, 19 Sep 2024 19:44:55 +0000
Labels:                 app=myapp
                        tier=frontend
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               app=frontend
Replicas:               5 desired | 5 updated | 5 total | 5 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=frontend
           version=v1
  Containers:
   webapp-color:
    Image:         kodekloud/webapp-color:v1
    Port:          <none>
    Host Port:     <none>
    Environment:   <none>
    Mounts:        <none>
  Volumes:         <none>
  Node-Selectors:  <none>
  Tolerations:     <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   frontend-6fd794f9f9 (5/5 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  61s   deployment-controller  Scaled up replica set frontend-6fd794f9f9 to 5

controlplane ~ ‚ûú  kubectl get svc
NAME               TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
frontend-service   NodePort    10.105.25.29   <none>        8080:30080/TCP   84s
kubernetes         ClusterIP   10.96.0.1      <none>        443/TCP          4m29s

controlplane ~ ‚ûú  kubectl describe svc frontend-service 
Name:                     frontend-service
Namespace:                default
Labels:                   app=myapp
Annotations:              <none>
Selector:                 app=frontend
Type:                     NodePort
IP Family Policy:         SingleStack
IP Families:              IPv4
IP:                       10.105.25.29
IPs:                      10.105.25.29
Port:                     <unset>  8080/TCP
TargetPort:               8080/TCP
NodePort:                 <unset>  30080/TCP
Endpoints:                10.244.0.4:8080,10.244.0.5:8080,10.244.0.6:8080 + 2 more...
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   <none>
-----------------------------------------------------------------------------------------
02. A new deployment called frontend-v2 has been created in the default namespace using the image kodekloud/webapp-color:v2. This deployment will be used to test a newer version of the same app.
Configure the deployment in such a way that the service called frontend-service routes less than 20% of traffic to the new deployment.
Do not increase the replicas of the frontend deployment.

controlplane ~ ‚ûú  kubectl get pod,deploy,svc
NAME                               READY   STATUS    RESTARTS   AGE
pod/frontend-6fd794f9f9-b5w5w      1/1     Running   0          14m
pod/frontend-6fd794f9f9-bbgmv      1/1     Running   0          14m
pod/frontend-6fd794f9f9-jw6zm      1/1     Running   0          14m
pod/frontend-6fd794f9f9-mfjbg      1/1     Running   0          14m
pod/frontend-6fd794f9f9-wcx47      1/1     Running   0          14m
pod/frontend-v2-7bdb47f6f7-9pf8w   1/1     Running   0          10m

NAME                          READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/frontend      5/5     5            5           14m
deployment.apps/frontend-v2   1/1     1            1           10m

NAME                       TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
service/frontend-service   NodePort    10.105.25.29   <none>        8080:30080/TCP   14m
service/kubernetes         ClusterIP   10.96.0.1      <none>        443/TCP          17m
controlplane ~ ‚ûú  kubectl edit deploy frontend-v2
replicas: 2 > replicas: 1

-----------------------------------------------------------------------------------------
Scale down the v1 version of the apps to 0 replicas and scale up the new(v2) version to 5 replicas.

kubectl scale deployment frontend --replicas=0 
kubectl scale deployment frontend-v2 --replicas=5
kubectl delete deployments frontend
deployment.apps "frontend" deleted
-----------------------------------------------------------------------------------------

6.12 Jobs 08:07
pod definition file: pod-math.yaml
apiVersion: v1
kind: Pod
metadata:
  name: math-pod
spec:
  - name: math-add
    image: ubuntu
    command: ['expr', '3', '+', '2']	
	
‚ñ† kubectl create -f  pod-math.yaml
‚ñ† kubectl get pod 
-> READY 0/1
   STATUS: completed
 
spec: > restartPolicy: Never / Always

do task for complete a function -> using Jobs

kubectl logs math-job-xxxx
5  -> stdiout

if you want to run multiple pods use define

spec: completions: <number>	  
spec: parallelism: <number> : create 3 pod in 1 time to complete the number of completions
	  
apiVersion: batch/v1
kind: Job
metadata:
  name: math-job
spec:
  template:
    spec:
	  container:
	    - name: math-add
		  image: ubuntu
		  command: ['expr', '3', '+', '2']
      restartPolicy: Never
	  completions: 3
	  parallelism: 3

6.13 Cron Jobs 01:45

Cron job is job that can be schedule

# ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ minute (0 - 59)
# ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ hour (0 - 23)
# ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ day of the month (1 - 31)
# ‚îÇ ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ month (1 - 12)
# ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ day of the week (0 - 6) (Sunday to Saturday)
# ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ                                   OR sun, mon, tue, wed, thu, fri, sat
# ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ 
# ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ
# * * * * *
For example, 0 3 * * 1 means this task is scheduled to run weekly on a Monday at 3 AM

apiVersion: batch/v1
kind: CronJob
metadata:
  name: math-schedule
spec:                          -> Cronjob spec
  schedule: 0 3 * * 1
  jobTemplate:
    spec:                      -> Job spec
      completions: 3
	  parallelism: 3
      template:
        spec:                  -> Pod spec
	      container:
	        - name: math-add
		      image: ubuntu
		      command: ['expr', '3', '+', '2']
          restartPolicy: Never

If you can't explain it simply you don't understand it well enough. - Einstein

6.14 Practice Test - Kubernetes - CKAD - Jobs and CronJobs
------------------------------------------------------------------------------------------
01.A pod definition file named throw-dice-pod.yaml is given. The image throw-dice randomly returns a value between 1 and 6. 6 is 
considered success and all others are failure. Try deploying the POD and view the POD logs for the generated number.
File is located at /root/throw-dice-pod.yaml
controlplane ~ ‚ûú  cat throw-dice-pod.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: throw-dice-pod
spec:
  containers:
  -  image: kodekloud/throw-dice
     name: throw-dice
  restartPolicy: Never
  
controlplane ~ ‚ûú  kubectl create -f throw-dice-pod.yaml
pod/throw-dice-pod created

controlplane ~ ‚ûú  k get pod
NAME             READY   STATUS   RESTARTS   AGE
throw-dice-pod   0/1     Error    0          10s

controlplane ~ ‚ûú k logs throw-dice-pod
3


------------------------------------------------------------------------------------------
Create a Job using this POD definition file or from the imperative command and look at how many attempts does it take to get a '6'.

Job Name: throw-dice-job
Image Name: kodekloud/throw-dice

Monitor and wait for the job to succeed. Throughout this practice test remember to increase the 'BackOffLimit' to prevent the job from quitting before it succeeds.

Check out the documentation page about the BackOffLimit property.

kubectl create job throw-dice-job --image=kodekloud/throw-dice --dry-run=client -o yaml  > throw-dice-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: throw-dice-job
spec:
  backoffLimit: 15 # This is so the job does not quit before it succeeds.
  template:
    spec:
      containers:
      - name: throw-dice
        image: kodekloud/throw-dice
      restartPolicy: Never
	  
------------------------------------------------------------------------------------------
Update the job definition to run as many times as required to get 2 successful 6's.
Delete existing job and create a new one with the given spec. Monitor and wait for the job to succeed.
Job Name: throw-dice-job
Image Name: kodekloud/throw-dice
Completions: 2
Job Succeeded: True
controlplane ~ ‚ûú  k get job,pod
NAME                       STATUS     COMPLETIONS   DURATION   AGE
job.batch/throw-dice-job   Complete   1/1           14s        3m34s

NAME                       READY   STATUS      RESTARTS   AGE
pod/throw-dice-job-f5986   0/1     Error       0          3m34s
pod/throw-dice-job-rpb8b   0/1     Completed   0          3m23s
pod/throw-dice-pod         0/1     Error       0          7m27s

controlplane ~ ‚ûú  k delete job throw-dice-job 
job.batch "throw-dice-job" deleted

controlplane ~ ‚ûú  k get job,pod
NAME                 READY   STATUS   RESTARTS   AGE
pod/throw-dice-pod   0/1     Error    0          7m46s

controlplane ~ ‚ûú  vi throw-dice-job.yaml 
controlplane ~ ‚ûú  cat throw-dice-job.yaml 
apiVersion: batch/v1
kind: Job
metadata:
  name: throw-dice-job
spec:
  backoffLimit: 15
  template:
    spec:
      containers:
        -  image: kodekloud/throw-dice
           name: throw-dice
      restartPolicy: Never
  completions: 2
  
controlplane ~ ‚ûú  k create -f throw-dice-job.yaml 
job.batch/throw-dice-job created

controlplane ~ ‚ûú  k get job,pod
NAME                       STATUS    COMPLETIONS   DURATION   AGE
job.batch/throw-dice-job   Running   0/2           11s        11s

NAME                       READY   STATUS   RESTARTS   AGE
pod/throw-dice-job-7v6v6   0/1     Error    0          11s
pod/throw-dice-pod         0/1     Error    0          13m

controlplane ~ ‚ûú  k get job,pod
NAME                       STATUS     COMPLETIONS   DURATION   AGE
job.batch/throw-dice-job   Complete   2/2           19s        21s

NAME                       READY   STATUS      RESTARTS   AGE
pod/throw-dice-job-2pz4g   0/1     Completed   0          9s
pod/throw-dice-job-7v6v6   0/1     Error       0          21s
pod/throw-dice-job-b4cft   0/1     Completed   0          6s
pod/throw-dice-pod         0/1     Error       0          13m



------------------------------------------------------------------------------------------
That took a while. Let us try to speed it up, by running upto 3 jobs in parallel.
Update the job definition to run 3 jobs in parallel.

controlplane ~ ‚ûú  cat throw-dice-job.yaml 
apiVersion: batch/v1
kind: Job
metadata:
  name: throw-dice-job
spec:
  backoffLimit: 15
  template:
    spec:
      containers:
        -  image: kodekloud/throw-dice
           name: throw-dice
      restartPolicy: Never
  completions: 3
  parallelism: 3
------------------------------------------------------------------------------------------

controlplane ~ ‚ûú  k get job,pod
NAME                       STATUS     COMPLETIONS   DURATION   AGE
job.batch/throw-dice-job   Complete   3/3           2m39s      5m41s

NAME                       READY   STATUS      RESTARTS   AGE
pod/throw-dice-job-42c2s   0/1     Error       0          4m55s
pod/throw-dice-job-6spm5   0/1     Error       0          5m41s
pod/throw-dice-job-bzgnw   0/1     Error       0          4m55s
pod/throw-dice-job-c964z   0/1     Error       0          4m33s
pod/throw-dice-job-c9t6k   0/1     Error       0          4m59s
pod/throw-dice-job-cpx5s   0/1     Error       0          3m12s
pod/throw-dice-job-dnrhd   0/1     Error       0          4m59s
pod/throw-dice-job-fm5zp   0/1     Error       0          5m41s
pod/throw-dice-job-kgpsz   0/1     Completed   0          4m59s
pod/throw-dice-job-lwjxm   0/1     Completed   0          3m6s
pod/throw-dice-job-qhw69   0/1     Completed   0          3m12s
pod/throw-dice-job-st5cw   0/1     Error       0          4m33s
pod/throw-dice-job-xp78s   0/1     Error       0          5m41s
pod/throw-dice-pod         0/1     Error       0          28m

----------------------------------------------------------
Let us now schedule that job to run at 21:30 hours every day.
Create a CronJob for this.

apiVersion: batch/v1
kind: CronJob
metadata:
  name: throw-dice-cron-job
spec:
  schedule: "30 21 * * *"
  jobTemplate:
    spec:
      completions: 3
      parallelism: 3
      backoffLimit: 25 # This is so the job does not quit before it succeeds.
      template:
        spec:
          containers:
          - name: throw-dice
            image: kodekloud/throw-dice
          restartPolicy: Never
		  
controlplane ~ ‚ûú  k create -f throw-dice-cronjob.yaml 
cronjob.batch/throw-dice-cron-job created

controlplane ~ ‚ûú  cat throw-dice-cronjob.yaml 
apiVersion: batch/v1
kind: CronJob
metadata:
  name: throw-dice-cron-job
spec:
  schedule: 30 21 * * *
  jobTemplate:
    spec:
      backoffLimit: 15
      template:
       spec:
         containers:
           - image: kodekloud/throw-dice
             name: throw-dice
         restartPolicy: Never
      completions: 3
      parallelism: 3

6.15 Solution - Jobs and Cronjobs (optional) 09:07
